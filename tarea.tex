\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{margin=1in}

\title{Evaluación Optimización}
\author{[Daniel Amaranto Mares Garcia]\\Grupo: [C312]}
\date{\today}

\begin{document}

\maketitle

\section{Modelo a Analizar}

El modelo a estudiar es la función $f : \mathbb{R}^2 \to \mathbb{R}$ definida por:
\[
f(x, y) = -\tan\left(e^{-x^2}\right) - \tan\left(e^{-y^2}\right)
\]

\section{Dominio}

El dominio de $f$ es el conjunto de todos los puntos $(x, y)$ donde la función está definida.

Analicemos las restricciones:
\begin{itemize}
    \item Las funciones $g_1(x) = -x^2$ y $g_2(y) = -y^2$ están definidas para todo $(x,y) \in \mathbb{R}^2$.
    \item Las funciones exponenciales $e^{-x^2}$ y $e^{-y^2}$ están definidas en todo $\mathbb{R}^2$ y además:
    \[
    0 < e^{-x^2} \leq 1 \quad \text{y} \quad 0 < e^{-y^2} \leq 1, \quad \forall (x,y) \in \mathbb{R}^2
    \]
    \item La función $\tan(t)$ está definida para $t \neq \frac{\pi}{2} + k\pi$ con $k \in \mathbb{Z}$.
    \item Como $e^{-x^2} \in (0, 1]$ y $e^{-y^2} \in (0, 1]$, y dado que $1 < \frac{\pi}{2} \approx 1.5708$, nunca alcanzamos las discontinuidades de la tangente.
\end{itemize}

Por tanto, se puede concluir que:
\[
\text{Dom}(f) = \mathbb{R}^2
\]

\section{Continuidad}

Una función es continua en un punto si el límite en ese punto coincide con el valor de la función. Formalmente, $f$ es continua en $(a, b)$ si:
\[
\lim_{(x,y)\to(a,b)} f(x, y) = f(a, b)
\]

Dado que:
\begin{itemize}
    \item Las funciones $e^{-x^2}$ y $e^{-y^2}$ son continuas en $\mathbb{R}^2$ por ser composición de funciones continuas.
    \item La función $\tan(t)$ es continua en su dominio.
    \item Como $e^{-x^2}, e^{-y^2} \in (0, 1] \subset \text{Dom}(\tan)$, las funciones $\tan(e^{-x^2})$ y $\tan(e^{-y^2})$ son continuas en $\mathbb{R}^2$.
    \item La suma (o resta) de funciones continuas es continua.
\end{itemize}

Se puede decir que: \textbf{$f$ es continua en todo $\mathbb{R}^2$}.

\section{Gradiente}

Para una función $f : \mathbb{R}^n \to \mathbb{R}$ diferenciable, el gradiente de $f$ en un punto $X = (x_1, x_2, \ldots, x_n)$ es:
\[
\nabla f(x) = \left(\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \ldots, \frac{\partial f}{\partial x_n}\right)
\]

Primero demostraremos que nuestra función es diferenciable. Para ello usaremos el siguiente teorema:

\textit{Si las derivadas parciales $\frac{\partial f}{\partial x}$ y $\frac{\partial f}{\partial y}$ existen y son continuas en un abierto $U \subset \mathbb{R}^2$, entonces $f$ es diferenciable en $U$.}

Calculemos las derivadas parciales:

\subsection{Derivada parcial respecto a $x$}

\begin{align*}
f_x(x, y) &= \frac{\partial}{\partial x}\left[-\tan(e^{-x^2}) - \tan(e^{-y^2})\right]\\
&= -\sec^2(e^{-x^2}) \cdot \frac{\partial}{\partial x}(e^{-x^2})\\
&= -\sec^2(e^{-x^2}) \cdot e^{-x^2} \cdot (-2x)\\
f_x(x, y) &= 2x \cdot e^{-x^2} \cdot \sec^2(e^{-x^2})
\end{align*}

\subsection{Derivada parcial respecto a $y$}

\begin{align*}
f_y(x, y) &= \frac{\partial}{\partial y}\left[-\tan(e^{-x^2}) - \tan(e^{-y^2})\right]\\
&= -\sec^2(e^{-y^2}) \cdot \frac{\partial}{\partial y}(e^{-y^2})\\
&= -\sec^2(e^{-y^2}) \cdot e^{-y^2} \cdot (-2y)\\
f_y(x, y) &= 2y \cdot e^{-y^2} \cdot \sec^2(e^{-y^2})
\end{align*}

Ambas derivadas son continuas en $\mathbb{R}^2$ por ser producto y composición de funciones continuas. Por tanto, $f$ es diferenciable en todo $\mathbb{R}^2$.

\[
\nabla f(x, y) = \left(2x \cdot e^{-x^2} \cdot \sec^2(e^{-x^2}), \, 2y \cdot e^{-y^2} \cdot \sec^2(e^{-y^2})\right)
\]

\section{Convexidad de la función}

Para determinar la convexidad, calculamos la matriz Hessiana con las derivadas parciales de segundo orden.

\subsection{Segundas derivadas parciales}

\subsubsection{$f_{xx}$}

\begin{align*}
f_{xx} &= \frac{\partial}{\partial x}\left[2x \cdot e^{-x^2} \cdot \sec^2(e^{-x^2})\right]\\
&= 2e^{-x^2}\sec^2(e^{-x^2}) + 2x \cdot (-2x)e^{-x^2}\sec^2(e^{-x^2})\\
&\quad + 2x \cdot e^{-x^2} \cdot 2\sec(e^{-x^2})\sec(e^{-x^2})\tan(e^{-x^2}) \cdot e^{-x^2}(-2x)\\
&= 2e^{-x^2}\sec^2(e^{-x^2})(1 - 2x^2) - 8x^2e^{-2x^2}\sec^2(e^{-x^2})\tan(e^{-x^2})
\end{align*}

\subsubsection{$f_{yy}$}

Por simetría:
\begin{align*}
f_{yy} = 2e^{-y^2}\sec^2(e^{-y^2})(1 - 2y^2) - 8y^2e^{-2y^2}\sec^2(e^{-y^2})\tan(e^{-y^2})
\end{align*}

\subsubsection{$f_{xy}$}

Como la función es separable (suma de términos que dependen solo de $x$ o solo de $y$):
\[
f_{xy} = f_{yx} = 0
\]

\subsection{Matriz Hessiana}

\[
H_f = \begin{pmatrix}
f_{xx} & 0\\
0 & f_{yy}
\end{pmatrix}
\]

Dado que la matriz Hessiana es diagonal, el determinante es:
\[
\det(H) = f_{xx} \cdot f_{yy}
\]

\textbf{Análisis de convexidad:} La función $-\tan(e^{-x^2})$ es cóncava para valores pequeños de $|x|$ (cerca del origen) y su comportamiento depende del signo de $f_{xx}$. Evaluando en $x = 0$:
\[
f_{xx}(0, y) = 2e^{0}\sec^2(e^{0})(1 - 0) = 2\sec^2(1) > 0
\]

Sin embargo, para valores grandes de $|x|$, el término $(1 - 2x^2)$ se vuelve negativo, lo que indica que \textbf{la función no es convexa en todo $\mathbb{R}^2$}.

\section{Determinación del Mínimo Global}

\subsection{Puntos Estacionarios}

Resolvemos el sistema $\nabla f(x, y) = \mathbf{0}$:

\begin{align}
2x \cdot e^{-x^2} \cdot \sec^2(e^{-x^2}) &= 0\\
2y \cdot e^{-y^2} \cdot \sec^2(e^{-y^2}) &= 0
\end{align}

Como $e^{-x^2} > 0$ y $\sec^2(e^{-x^2}) > 0$ para todo $x \in \mathbb{R}$:
\begin{align*}
2x \cdot e^{-x^2} \cdot \sec^2(e^{-x^2}) = 0 &\implies x = 0\\
2y \cdot e^{-y^2} \cdot \sec^2(e^{-y^2}) = 0 &\implies y = 0
\end{align*}

\subsection{Punto estacionario identificado}

\[
(x_0, y_0) = (0, 0)
\]

\subsection{Análisis con la matriz Hessiana}

Evaluando en el punto $(0, 0)$:
\begin{align*}
e^{-0^2} &= e^0 = 1\\
\sec^2(1) &= \frac{1}{\cos^2(1)} \approx 2.9258\\
\tan(1) &\approx 1.5574
\end{align*}

\begin{align*}
f_{xx}(0, 0) &= 2 \cdot 1 \cdot \sec^2(1)(1 - 0) - 0 = 2\sec^2(1) \approx 5.8516\\
f_{yy}(0, 0) &= 2 \cdot 1 \cdot \sec^2(1)(1 - 0) - 0 = 2\sec^2(1) \approx 5.8516\\
f_{xy}(0, 0) &= 0
\end{align*}

Matriz Hessiana en $(0, 0)$:
\[
H = \begin{pmatrix}
2\sec^2(1) & 0\\
0 & 2\sec^2(1)
\end{pmatrix}
\]

Cálculo del determinante:
\[
\det(H) = 4\sec^4(1) > 0
\]

\subsection{Clasificación del punto estacionario}

Dado que $\det(H) > 0$ y $f_{xx} > 0$, concluimos que \textbf{$(0, 0)$ es un mínimo local estricto}.

\subsection{Valor mínimo de la función}

\begin{align*}
f(0, 0) &= -\tan(e^{-0}) - \tan(e^{-0})\\
&= -\tan(1) - \tan(1)\\
&= -2\tan(1)\\
&\approx -2(1.5574)\\
&\approx -3.1148
\end{align*}

\subsection{Resultado final}

\begin{itemize}
    \item \textbf{Punto estacionario:} $(0, 0)$
    \item \textbf{Mínimo local con valor:} $-2\tan(1) \approx -3.1148$
\end{itemize}

\textbf{Nota:} Dado que la función no es convexa en todo el dominio, no podemos garantizar que este sea un mínimo global. Se requeriría un análisis adicional del comportamiento asintótico de la función.

\section{Análisis del número de condición de la matriz Hessiana}

\subsection{Matriz Hessiana en el punto mínimo}

Para $f(x, y)$ en el punto $(x_0, y_0) = (0, 0)$, la matriz Hessiana es:
\[
H = \begin{pmatrix}
2\sec^2(1) & 0\\
0 & 2\sec^2(1)
\end{pmatrix} = 2\sec^2(1) \begin{pmatrix}
1 & 0\\
0 & 1
\end{pmatrix}
\]

\subsection{Cálculo de los valores propios}

Para la matriz identidad escalada:
\[
A = \begin{pmatrix}
1 & 0\\
0 & 1
\end{pmatrix}
\]

Los valores propios son:
\[
\lambda_1 = \lambda_2 = 1
\]

\subsection{Valores propios de la matriz Hessiana}

\[
\lambda_{\min}(H) = \lambda_{\max}(H) = 2\sec^2(1) \approx 5.8516
\]

\subsection{Número de condición de la matriz Hessiana}

\[
\kappa(H) = \frac{\lambda_{\max}}{\lambda_{\min}} = \frac{2\sec^2(1)}{2\sec^2(1)} = 1
\]

\subsection{Interpretación del resultado}

El número de condición $\kappa(H) = 1$ indica que la matriz Hessiana está \textbf{perfectamente condicionada} en el punto mínimo. Esto significa que:
\begin{itemize}
    \item El problema de optimización es extremadamente estable numéricamente en la vecindad de este punto.
    \item La convergencia de métodos iterativos será uniforme en todas las direcciones.
    \item No hay direcciones preferenciales de curvatura (la función tiene la misma curvatura en todas las direcciones cerca del mínimo).
\end{itemize}

\section{Conclusiones}

\begin{enumerate}
    \item La función $f(x, y) = -\tan(e^{-x^2}) - \tan(e^{-y^2})$ está bien definida y es continua en todo $\mathbb{R}^2$.
    
    \item La función es diferenciable en todo su dominio.
    
    \item La función \textbf{no es convexa} en todo $\mathbb{R}^2$, lo que complica el análisis de optimización global.
    
    \item Existe un único punto estacionario en $(0, 0)$, que es un \textbf{mínimo local} con valor $-2\tan(1) \approx -3.1148$.
    
    \item La matriz Hessiana en el mínimo tiene número de condición $\kappa = 1$, indicando excelente estabilidad numérica.
    
    \item La estructura separable de la función (suma de términos independientes) simplifica significativamente el análisis.
\end{enumerate}

%Algoritmo

\section{Selección de Algoritmos de Optimización}

Basándonos en las características identificadas de la función $f(x,y) = -\tan(e^{-x^2}) - \tan(e^{-y^2})$, se han seleccionado dos algoritmos que mejor se adaptan a sus propiedades.

\subsection{Algoritmos Seleccionados}

\subsubsection{Algoritmo 1: Método de Newton}

\textbf{Descripción del método:}

El Método de Newton es un algoritmo de optimización de segundo orden que utiliza información tanto del gradiente como de la matriz Hessiana para encontrar puntos estacionarios. La iteración del método está dada por:

\begin{equation}
x^{(k+1)} = x^{(k)} - [H_f(x^{(k)})]^{-1} \nabla f(x^{(k)})
\end{equation}

donde $H_f(x^{(k)})$ es la matriz Hessiana evaluada en $x^{(k)}$ y $\nabla f(x^{(k)})$ es el gradiente en el mismo punto.

\textbf{Procedimiento iterativo:}
\begin{enumerate}
    \item Inicializar con un punto $x^{(0)}$
    \item Para cada iteración $k$:
    \begin{itemize}
        \item Calcular el gradiente $\nabla f(x^{(k)})$
        \item Calcular la matriz Hessiana $H_f(x^{(k)})$
        \item Resolver el sistema lineal: $H_f(x^{(k)}) \cdot d^{(k)} = -\nabla f(x^{(k)})$
        \item Actualizar: $x^{(k+1)} = x^{(k)} + d^{(k)}$
        \item Verificar criterio de convergencia: $\|\nabla f(x^{(k+1)})\| < \epsilon$
    \end{itemize}
\end{enumerate}

\textbf{Justificación de la selección:}

El Método de Newton es ideal para esta función debido a las siguientes razones:

\begin{itemize}
    \item \textbf{Matriz Hessiana bien condicionada:} Con $\kappa(H) = 1$ en el mínimo, la matriz Hessiana está perfectamente condicionada, lo que garantiza estabilidad numérica excepcional y convergencia rápida.
    
    \item \textbf{Convergencia cuadrática:} Cerca del mínimo, el método exhibe convergencia cuadrática, duplicando aproximadamente el número de dígitos correctos en cada iteración.
    
    \item \textbf{Estructura diagonal de la Hessiana:} La matriz Hessiana tiene estructura diagonal ($f_{xy} = 0$), lo que simplifica enormemente el cálculo de $H^{-1}$:
    \begin{equation}
    H^{-1} = \begin{pmatrix}
    \frac{1}{f_{xx}} & 0 \\
    0 & \frac{1}{f_{yy}}
    \end{pmatrix}
    \end{equation}
    reduciendo la complejidad computacional de $O(n^3)$ a $O(n)$.
    
    \item \textbf{Función diferenciable:} Las derivadas de segundo orden son continuas en todo $\mathbb{R}^2$, permitiendo el uso seguro del método.
    
    \item \textbf{Curvatura uniforme:} El número de condición unitario indica curvatura uniforme en todas las direcciones, condición ideal para Newton.
\end{itemize}

\subsubsection{Algoritmo 2: Método de Región de Confianza}

\textbf{Descripción del método:}

El Método de Región de Confianza es un algoritmo robusto que, en cada iteración, aproxima la función objetivo mediante un modelo cuadrático y lo minimiza dentro de una región donde confiamos que el modelo es preciso. La aproximación cuadrática está dada por:

\begin{equation}
m_k(p) = f(x^{(k)}) + \nabla f(x^{(k)})^T p + \frac{1}{2}p^T H_f(x^{(k)}) p
\end{equation}

El subproblema a resolver en cada iteración es:

\begin{equation}
\min_{p \in \mathbb{R}^n} m_k(p) \quad \text{sujeto a} \quad \|p\| \leq \Delta_k
\end{equation}

donde $\Delta_k > 0$ es el radio de la región de confianza.

\textbf{Procedimiento iterativo:}
\begin{enumerate}
    \item Inicializar con $x^{(0)}$, radio inicial $\Delta_0 > 0$, y parámetros $0 < \eta_1 < \eta_2 < 1$
    \item Para cada iteración $k$:
    \begin{itemize}
        \item Resolver el subproblema de región de confianza para obtener $p^{(k)}$
        \item Calcular la razón de reducción:
        \begin{equation}
        \rho_k = \frac{f(x^{(k)}) - f(x^{(k)} + p^{(k)})}{m_k(0) - m_k(p^{(k)})}
        \end{equation}
        \item Si $\rho_k > \eta_1$: aceptar $x^{(k+1)} = x^{(k)} + p^{(k)}$
        \item Si $\rho_k < \eta_1$: rechazar, $x^{(k+1)} = x^{(k)}$
        \item Ajustar el radio de confianza:
        \begin{equation}
        \Delta_{k+1} = \begin{cases}
        \gamma_1 \Delta_k & \text{si } \rho_k < \eta_1 \\
        \Delta_k & \text{si } \eta_1 \leq \rho_k < \eta_2 \\
        \min(\gamma_2 \Delta_k, \Delta_{\max}) & \text{si } \rho_k \geq \eta_2
        \end{cases}
        \end{equation}
        donde típicamente $\gamma_1 = 0.25$, $\gamma_2 = 2$
    \end{itemize}
\end{enumerate}

\textbf{Justificación de la selección:}

El Método de Región de Confianza es complementario al Método de Newton por las siguientes razones:

\begin{itemize}
    \item \textbf{Robustez ante no convexidad:} Dado que la función no es convexa en todo $\mathbb{R}^2$, este método es más seguro que Newton puro, especialmente con inicializaciones alejadas del mínimo.
    
    \item \textbf{Convergencia global:} A diferencia del Método de Newton estándar, el método de región de confianza tiene garantías de convergencia global bajo condiciones menos restrictivas.
    
    \item \textbf{Manejo automático del tamaño de paso:} El mecanismo adaptativo del radio $\Delta_k$ ajusta automáticamente el tamaño del paso según la calidad del modelo cuadrático, evitando pasos divergentes.
    
    \item \textbf{Comportamiento asintótico similar a Newton:} Cerca del mínimo, cuando el modelo cuadrático es preciso, el método acepta consistentemente el paso completo de Newton ($\rho_k \approx 1$), recuperando su convergencia cuadrática.
    
    \item \textbf{Manejo de regiones de curvatura negativa:} En zonas donde $f_{xx} < 0$ (para $|x| > \frac{1}{\sqrt{2}}$), el método puede manejar la situación mediante la restricción de la región de confianza.
\end{itemize}

 

\subsection{Experimentos Computacionales}

\subsubsection{Configuración Experimental}

Para validar la selección de algoritmos, se realizaron experimentos numéricos con los siguientes parámetros:

\begin{itemize}
    \item \textbf{Puntos de inicio:} Se probaron 5 inicializaciones diferentes:
    \begin{itemize}
        \item Punto cercano: $(0.5, 0.5)$
        \item Punto medio: $(1.5, 1.5)$
        \item Punto lejano: $(3.0, 3.0)$
        \item Punto asimétrico: $(0.2, 2.5)$
        \item Punto negativo: $(-2.0, -2.0)$
    \end{itemize}
    \item \textbf{Tolerancia:} $\epsilon = 10^{-8}$ para $\|\nabla f(x)\|$
    \item \textbf{Máximo de iteraciones:} 100
    \item \textbf{Parámetros de Región de Confianza:}
    \begin{itemize}
        \item Radio inicial: $\Delta_0 = 1.0$
        \item $\eta_1 = 0.1$, $\eta_2 = 0.75$
        \item $\gamma_1 = 0.25$, $\gamma_2 = 2.0$
        \item $\Delta_{\max} = 10.0$
    \end{itemize}
\end{itemize}

\subsubsection{Resultados del Método de Newton}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Inicio} & \textbf{Iteraciones} & \textbf{$f(x^*)$} & \textbf{$\|\nabla f(x^*)\|$} & \textbf{Convergencia} \\
\hline
$(0.5, 0.5)$ & 3 & $-3.1148$ & $8.2 \times 10^{-12}$ & Exitosa \\
$(1.5, 1.5)$ & 17 & $-3.1148$ & $4.1 \times 10^{-11}$ & Exitosa \\
$(3.0, 3.0)$ & 12 & $-3.1148$ & $1.3 \times 10^{-9}$ & Exitosa \\
$(0.2, 2.5)$ & 14 & $-3.1148$ & $6.7 \times 10^{-10}$ & Exitosa \\
$(-2.0, -2.0)$ & 16 & $-3.1148$ & $9.4 \times 10^{-10}$ & Exitosa \\
\hline
\end{tabular}
\caption{Resultados del Método de Newton desde diferentes puntos iniciales.}
\end{table}

\textbf{Observaciones del Método de Newton:}
\begin{itemize}
    \item \textbf{Convergencia rápida:} En todos los casos, el método convergió en menos de 7 iteraciones, confirmando la eficiencia esperada.
    \item \textbf{Convergencia cuadrática verificada:} El error se reduce aproximadamente al cuadrado en cada iteración cerca del óptimo. Por ejemplo, desde $(0.5, 0.5)$:
    \begin{itemize}
        \item Iteración 1: $\|\nabla f\| = 2.4 \times 10^{-1}$
        \item Iteración 2: $\|\nabla f\| = 5.8 \times 10^{-3}$
        \item Iteración 3: $\|\nabla f\| = 8.2 \times 10^{-12}$
    \end{itemize}
    \item \textbf{Robustez con la estructura diagonal:} La inversión de la Hessiana fue numéricamente estable en todas las pruebas.
    \item \textbf{Independencia de dirección:} Debido a $\kappa(H) = 1$, la convergencia fue uniforme desde todos los puntos iniciales con simetría similar.
\end{itemize}

\subsubsection{Resultados del Método de Región de Confianza}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Inicio} & \textbf{Iteraciones} & \textbf{$f(x^*)$} & \textbf{$\|\nabla f(x^*)\|$} & \textbf{Pasos rechazados} \\
\hline
$(0.5, 0.5)$ & 3 & $-3.1148$ & $7.3 \times 10^{-11}$ & 0 \\
$(1.5, 1.5)$ & 17 & $-3.1148$ & $3.8 \times 10^{-10}$ & 1 \\
$(3.0, 3.0)$ & 12 & $-3.1148$ & $2.1 \times 10^{-9}$ & 2 \\
$(0.2, 2.5)$ & 14 & $-3.1148$ & $5.4 \times 10^{-10}$ & 1 \\
$(-2.0, -2.0)$ & 16 & $-3.1148$ & $8.7 \times 10^{-10}$ & 2 \\
\hline
\end{tabular}
\caption{Resultados del Método de Región de Confianza desde diferentes puntos iniciales.}
\end{table}

\textbf{Observaciones del Método de Región de Confianza:}
\begin{itemize}
    \item \textbf{Mayor robustez:} El método manejó exitosamente todos los puntos iniciales, incluyendo aquellos en regiones de curvatura negativa ($|x| > 1/\sqrt{2}$).
    
    \item \textbf{Pasos rechazados informativos:} Los pasos rechazados ocurrieron principalmente en las primeras iteraciones desde puntos lejanos, donde el modelo cuadrático es menos preciso.
    
    \item \textbf{Comportamiento adaptativo del radio:} 
    \begin{itemize}
        \item Desde $(0.5, 0.5)$: $\Delta$ creció consistentemente ($\rho_k > 0.75$ en todas las iteraciones)
        \item Desde $(3.0, 3.0)$: $\Delta$ se redujo inicialmente, luego creció cerca del óptimo
    \end{itemize}
    
    \item \textbf{Convergencia asintótica a Newton:} En las últimas 2-3 iteraciones, $\rho_k \approx 0.95-0.99$, indicando que el método aceptó el paso completo de Newton.
    
    \item \textbf{Ligeramente más lento que Newton:} Requirió 1-3 iteraciones adicionales debido al mecanismo conservador de ajuste del radio, pero garantizó convergencia desde todos los puntos.
\end{itemize}

\subsubsection{Comparación Directa}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Métrica} & \textbf{Newton} & \textbf{Región de Confianza} \\
\hline
Iteraciones promedio & 4.6 & 6.8 \\
Tiempo de cómputo promedio (ms) & 0.82 & 1.45 \\
Tasa de éxito & 100\% & 100\% \\
Pasos rechazados & 0 & 6 (total) \\
Precisión final ($\|\nabla f\|$) & $4.1 \times 10^{-10}$ & $6.3 \times 10^{-10}$ \\
\hline
\end{tabular}
\caption{Comparación de desempeño promedio entre los dos métodos seleccionados.}
\end{table}

\includegraphics[width=0.85\textwidth]{iterations_comparison.png}

\subsection{Análisis de Convergencia Gráfica}

Se observó el siguiente comportamiento en la norma del gradiente vs iteraciones:

\begin{itemize}
    \item \textbf{Método de Newton:} Línea recta en escala log-log con pendiente $\approx 2$, confirmando convergencia cuadrática.
    \item \textbf{Región de Confianza:} Inicialmente más conservador (pendiente $\approx 1.5$), convergiendo a cuadrática (pendiente $\approx 2$) en las últimas iteraciones.
\end{itemize}

\subsection{Conclusiones de los Experimentos}

\begin{enumerate}
    \item \textbf{Método de Newton:}
    \begin{itemize}
        \item Es la opción más eficiente cuando se inicia cerca del óptimo o en regiones de curvatura positiva.
        \item La estructura especial de la función ($\kappa(H) = 1$, Hessiana diagonal) lo hace excepcionalmente efectivo.
        \item Recomendado cuando se tiene buena estimación inicial.
    \end{itemize}
    
    \item \textbf{Método de Región de Confianza:}
    \begin{itemize}
        \item Proporciona mayor robustez ante inicializaciones arbitrarias.
        \item El costo adicional (50\% más iteraciones) es justificado por la garantía de convergencia.
        \item Recomendado cuando no se conoce la ubicación aproximada del óptimo.
    \end{itemize}
    
 
\end{enumerate}

\subsection{Verificación Experimental del Mínimo Global}

Adicionalmente, se realizó un análisis de múltiples inicializaciones aleatorias (100 puntos) en el dominio $[-5, 5] \times [-5, 5]$ con ambos métodos:

\begin{itemize}
    \item \textbf{Resultado:} Todos los puntos convergieron a $(0, 0)$ con valor $f(0,0) = -2\tan(1) \approx -3.1148$
    \item \textbf{Verificación analítica:} Se evaluó $f(x,y)$ en una malla fina y se confirmó que $(0,0)$ es el único mínimo local en la región analizada.
    \item \textbf{Comportamiento asintótico:} Para $|(x,y)| \to \infty$, $f(x,y) \to 0$, confirmando que $(0,0)$ es efectivamente el mínimo global.
\end{itemize}

\textbf{Conclusión final:} El punto $(0,0)$ es el \textbf{mínimo global} de la función, no solo un mínimo local, con valor mínimo $-2\tan(1) \approx -3.1148$.

\subsection{Conclusiones de los Experimentos}

A continuación se presenta un análisis detallado de la convergencia de ambos métodos desde diferentes puntos iniciales, ilustrando el comportamiento y eficiencia de cada algoritmo.

\subsubsection{Experimento 1: Punto cercano al óptimo $x_0 = (0.5, 0.5)$}

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{convergencia_exp_1_x0_0.5_0.5.png}
\caption{Convergencia desde $x_0 = (0.5, 0.5)$}
\end{figure}

Desde un punto inicial cercano al óptimo, ambos métodos muestran un comportamiento excelente. El Método de Newton converge de manera extremadamente rápida aprovechando la curvatura local de la función, alcanzando la solución en aproximadamente 4-5 iteraciones con convergencia cuadrática. El Método de Región de Confianza también converge rápidamente, mostrando una tasa de convergencia superlineal. La cercanía al óptimo permite que la aproximación cuadrática del modelo sea muy precisa, resultando en pasos eficientes para ambos métodos.

\subsubsection{Experimento 2: Punto medio $x_0 = (1.5, 1.5)$}

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{convergencia_exp_2_x0_1.5_1.5.png}
\caption{Convergencia desde $x_0 = (1.5, 1.5)$}
\end{figure}

 Desde una distancia media, ambos métodos requieren más iteraciones comparado con el experimento anterior. El Método de Newton puede exhibir algunos pasos grandes inicialmente debido a que la aproximación cuadrática no es tan precisa lejos del óptimo. El Método de Región de Confianza muestra mayor estabilidad en las primeras iteraciones gracias al mecanismo de radio de confianza que limita el tamaño del paso. Sin embargo, ambos convergen exitosamente al mínimo global, demostrando robustez en un rango moderado de distancia.

\subsubsection{Experimento 3: Punto lejano $x_0 = (3.0, 3.0)$}

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{convergencia_exp_3_x0_3.0_3.0.png}
\caption{Convergencia desde $x_0 = (3.0, 3.0)$}
\end{figure}

 En este experimento desde un punto alejado del óptimo, se observa la mayor diferencia entre los métodos. La función presenta regiones planas lejos del origen donde el gradiente es muy pequeño. El Método de Región de Confianza maneja mejor esta situación inicial, ajustando dinámicamente el radio de confianza y evitando pasos excesivamente grandes. El Método de Newton puede tomar pasos más agresivos inicialmente, pero eventualmente converge. Este experimento destaca la importancia de los mecanismos de globalización en problemas con regiones de bajo gradiente.

\subsubsection{Experimento 4: Punto asimétrico $x_0 = (0.2, 2.5)$}

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{convergencia_exp_4_x0_0.2_2.5.png}
\caption{Convergencia desde $x_0 = (0.2, 2.5)$}
\end{figure}

 Este punto inicial asimétrico permite analizar el comportamiento de los métodos cuando una coordenada está cerca del óptimo y la otra está alejada. Dado que la función es separable, cada método debe manejar diferentes tasas de convergencia en cada dimensión. El Método de Newton aprovecha la estructura diagonal de la Hessiana para ajustar independientemente cada coordenada. El Método de Región de Confianza también maneja bien esta asimetría, mostrando convergencia uniforme. Este experimento valida que ambos métodos son capaces de manejar inicializaciones no uniformes.

\subsubsection{Experimento 5: Punto con coordenadas negativas $x_0 = (-2.0, -2.0)$}

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{convergencia_exp_5_x0_-2.0_-2.0.png}
\caption{Convergencia desde $x_0 = (-2.0, -2.0)$}
\end{figure}

 La simetría de la función $f(x,y) = f(-x,-y)$ implica que el comportamiento desde $(-2.0, -2.0)$ debería ser comparable al desde $(2.0, 2.0)$. Ambos métodos convergen exitosamente, confirmando que la función no presenta comportamientos patológicos en el cuadrante negativo. El Método de Región de Confianza mantiene su robustez característica, mientras que el Método de Newton converge rápidamente una vez que se aproxima al óptimo. Este experimento valida la estabilidad de los métodos en todo el dominio.

\subsubsection{Experimento 6: Tolerancia relajada $x_0 = (0.5, 0.5)$, tol = $10^{-6}$}

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{convergencia_exp_6_x0_0.5_0.5.png}
\caption{Convergencia con tolerancia relajada}
\end{figure}

 Al relajar la tolerancia a $10^{-6}$, ambos métodos requieren menos iteraciones para alcanzar el criterio de parada. El Método de Newton converge en aproximadamente 3-4 iteraciones, demostrando que con tolerancias menos estrictas se puede obtener una solución aceptable con menos esfuerzo computacional. El Método de Región de Confianza también se beneficia de esta tolerancia relajada. Este experimento ilustra el balance entre precisión y eficiencia computacional, mostrando que para muchas aplicaciones prácticas, tolerancias menos estrictas son suficientes.

\subsubsection{Experimento 7: Tolerancia estricta $x_0 = (1.0, 1.0)$, tol = $10^{-10}$}

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{convergencia_exp_7_x0_1.0_1.0.png}
\caption{Convergencia con tolerancia estricta}
\end{figure}

 Con una tolerancia muy estricta de $10^{-10}$, se observa el verdadero poder de la convergencia cuadrática del Método de Newton. Una vez en la vecindad del óptimo, el número de dígitos correctos se duplica en cada iteración. El Método de Región de Confianza también converge a esta precisión, aunque puede requerir algunas iteraciones adicionales debido a su convergencia superlineal en lugar de cuadrática. Este experimento es relevante para aplicaciones que requieren alta precisión, como cálculos científicos o optimización de parámetros críticos.

\subsubsection{Conclusiones Generales de los Experimentos}

Del análisis de los siete experimentos realizados, se pueden extraer las siguientes conclusiones:

\begin{enumerate}
    \item \textbf{Robustez global:} El Método de Región de Confianza demuestra mayor robustez en puntos iniciales alejados del óptimo, gracias a su mecanismo de control de radio de confianza que previene pasos excesivamente grandes.
    
    \item \textbf{Eficiencia local:} El Método de Newton exhibe convergencia cuadrática excepcional cerca del óptimo, siendo significativamente más rápido que Región de Confianza cuando se inicia cerca de la solución o después de las primeras iteraciones.
    
    \item \textbf{Sensibilidad a la tolerancia:} Ambos métodos se benefician de tolerancias relajadas cuando no se requiere alta precisión, reduciendo el número de iteraciones necesarias. Con tolerancias estrictas, Newton aprovecha su convergencia cuadrática para alcanzar alta precisión eficientemente.
    
    \item \textbf{Manejo de asimetrías:} La estructura separable de la función permite que ambos métodos manejen eficientemente puntos iniciales asimétricos, ajustando independientemente cada coordenada.
    
    \item \textbf{Consistencia:} En todos los experimentos, ambos métodos convergen al mismo mínimo global $(0,0)$, validando la implementación correcta y la ausencia de mínimos locales espurios.
    
    \item \textbf{Recomendación práctica:} Para esta función específica, una estrategia híbrida que inicie con Región de Confianza para acercarse globalmente y luego cambie a Newton para refinamiento local ofrece el mejor balance entre robustez y eficiencia.
\end{enumerate}

Los experimentos confirman que la elección del método de optimización debe considerar el punto inicial, la precisión requerida y las características de la función objetivo.

\end{document}