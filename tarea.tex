\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{url}
\geometry{margin=1in}

\title{Evaluación Optimización}
\author{[Daniel Amaranto Mares Garcia]\\Grupo: [C312]}


\begin{document}


\maketitle

\url{https://github.com/Amarantomc/Investigacion_AlgoritmosDeOptimizacion.git}
\section{Modelo a Analizar}

El modelo a estudiar es la función $f : \mathbb{R}^2 \to \mathbb{R}$ definida por:
\[
f(x, y) = -\tan\left(e^{-x^2}\right) - \tan\left(e^{-y^2}\right)
\]

\begin{figure}[h]
\centering
\includegraphics[width=0.85\textwidth]{geogebra-export.png}
\caption{Grafica 3D}
\end{figure}


\section{Propiedades Analíticas}

\subsection{Dominio y continuidad}

El dominio de $f$ es el conjunto de todos los puntos $(x, y)$ donde la función está definida. Las funciones $g_1(x) = -x^2$ y $g_2(y) = -y^2$ están definidas para todo $(x,y) \in \mathbb{R}^2$, y las exponenciales $e^{-x^2}$ y $e^{-y^2}$ satisfacen $0 < e^{-x^2} \leq 1$ y $0 < e^{-y^2} \leq 1$ en todo el plano. Dado que la función tangente está definida para $t \neq \frac{\pi}{2} + k\pi$ con $k \in \mathbb{Z}$, y como $e^{-x^2}, e^{-y^2} \in (0, 1] \subset [0, \frac{\pi}{2})$, las composiciones $\tan(e^{-x^2})$ y $\tan(e^{-y^2})$ nunca alcanzan las discontinuidades de la tangente. Por tanto, $\text{Dom}(f) = \mathbb{R}^2$.

La continuidad de $f$ se deduce del hecho que $e^{-x^2}$ y $e^{-y^2}$ son composiciones de funciones continuas en $\mathbb{R}^2$, y dado que sus imágenes están contenidas en el dominio de continuidad de $\tan(t)$, las composiciones $\tan(e^{-x^2})$ y $\tan(e^{-y^2})$ son continuas en todo $\mathbb{R}^2$. Por linealidad, $f$ es continua en $\mathbb{R}^2$.

\subsection{Diferenciabilidad y gradiente}

Para establecer la diferenciabilidad de $f$, calculamos sus derivadas parciales. Dado que $f$ es la suma de funciones compuestas de funciones diferenciables, las derivadas parciales existen y son continuas en $\mathbb{R}^2$, lo cual garantiza que $f$ es diferenciable en todo su dominio. Las derivadas parciales son:

\begin{align*}
f_x(x, y) &= 2x \cdot e^{-x^2} \cdot \sec^2(e^{-x^2})\\
f_y(x, y) &= 2y \cdot e^{-y^2} \cdot \sec^2(e^{-y^2})
\end{align*}

El gradiente de $f$ está dado por:
\[
\nabla f(x, y) = \left(2x \cdot e^{-x^2} \cdot \sec^2(e^{-x^2}), \, 2y \cdot e^{-y^2} \cdot \sec^2(e^{-y^2})\right)
\]

\subsection{Análisis de convexidad}

Para determinar la convexidad, calculamos la matriz Hessiana. Las segundas derivadas parciales son:

\begin{align*}
f_{xx} &= 2e^{-x^2}\sec^2(e^{-x^2})(1 - 2x^2) - 8x^2e^{-2x^2}\sec^2(e^{-x^2})\tan(e^{-x^2})\\
f_{yy} &= 2e^{-y^2}\sec^2(e^{-y^2})(1 - 2y^2) - 8y^2e^{-2y^2}\sec^2(e^{-y^2})\tan(e^{-y^2})\\
f_{xy} &= f_{yx} = 0
\end{align*}

Dada la estructura separable de la función, la matriz Hessiana es diagonal:
\[
H_f = \begin{pmatrix}
f_{xx} & 0\\
0 & f_{yy}
\end{pmatrix}, \quad \det(H_f) = f_{xx} \cdot f_{yy}
\]

El comportamiento de convexidad depende del signo de las segundas derivadas. Analizando la expresión de $f_{xx}$, el término $(1 - 2x^2)$ determina el signo: para $|x| < \frac{1}{\sqrt{2}} \approx 0.707$ se tiene $f_{xx} > 0$, mientras que para $|x| > \frac{1}{\sqrt{2}}$ se obtiene $f_{xx} < 0$. Por simetría, el mismo análisis aplica para $f_{yy}$ respecto a $y$. En consecuencia, la función es convexa en el cuadrado $\left[-\frac{1}{2}, \frac{1}{2}\right]^2$ que contiene el mínimo, y se vuelve cóncava fuera de esta región cuando $|x| > \frac{1}{\sqrt{2}}$ o $|y| > \frac{1}{\sqrt{2}}$.

\section{Determinación del Mínimo Global}

Para encontrar los puntos críticos, resolvemos el sistema $\nabla f(x, y) = \mathbf{0}$. Dado que $e^{-x^2} > 0$ y $\sec^2(e^{-x^2}) > 0$ para todo $x \in \mathbb{R}$, las ecuaciones $2x \cdot e^{-x^2} \cdot \sec^2(e^{-x^2}) = 0$ y $2y \cdot e^{-y^2} \cdot \sec^2(e^{-y^2}) = 0$ implican $x = 0$ e $y = 0$, respectivamente. Por tanto, $(x_0, y_0) = (0, 0)$ es el único punto estacionario.

Evaluando la matriz Hessiana en el punto $(0, 0)$:
\begin{align*}
f_{xx}(0, 0) &= 2\sec^2(1) \approx 5.8516, \quad f_{yy}(0, 0) = 2\sec^2(1) \approx 5.8516, \quad f_{xy}(0, 0) = 0
\end{align*}

La matriz Hessiana en $(0, 0)$ es $H = 2\sec^2(1) \cdot I_2$ con $\det(H) = 4\sec^4(1) > 0$. Dado que $\det(H) > 0$ y $f_{xx} > 0$, el punto $(0, 0)$ es un mínimo local estricto con valor $f(0,0) = -2\tan(1) \approx -3.1148$.

\subsection{Comportamiento asintótico y minimalidad global}

Dado que la función no es convexa en todo el dominio, se requiere analizar el comportamiento asintótico para establecer la minimalidad global. Por la estructura separable $f(x,y) = -\tan(e^{-x^2}) - \tan(e^{-y^2})$, cuando $x \to \pm\infty$ se tiene $e^{-x^2} \to 0$ y por tanto $\tan(e^{-x^2}) \to 0$, lo cual implica $-\tan(e^{-x^2}) \to 0$. El mismo análisis aplica para $y \to \pm\infty$.

Los límites direccionales relevantes son:
\begin{align}
\lim_{(x,y) \to (\infty,\infty)} f(x,y) &= 0\\
\lim_{y \to \infty} f(0,y) &= -\tan(1) \approx -1.5574\\
\lim_{x \to \infty} f(x,0) &= -\tan(1) \approx -1.5574
\end{align}

Comparando estos valores con $f(0,0) = -2\tan(1) \approx -3.1148$, se verifica que $f(0,0) < f(0,\infty) < f(\infty,\infty)$. Por continuidad y diferenciabilidad en todo $\mathbb{R}^2$, se concluye que $(0,0)$ es el mínimo global de la función.

\subsection{Número de condición de la matriz Hessiana}

La matriz Hessiana en el punto mínimo tiene la forma $H = 2\sec^2(1) \cdot I_2$, donde $I_2$ denota la matriz identidad de orden 2. Los valores propios son $\lambda_1 = \lambda_2 = 2\sec^2(1) \approx 5.8516$, de modo que el número de condición es:
\[
\kappa(H) = \frac{\lambda_{\max}}{\lambda_{\min}} = \frac{2\sec^2(1)}{2\sec^2(1)} = 1
\]

Este resultado indica que la matriz Hessiana está perfectamente condicionada en el punto mínimo, lo cual implica estabilidad numérica excepcional del problema de optimización en la vecindad de este punto, convergencia uniforme de métodos iterativos en todas las direcciones, y ausencia de direcciones preferenciales de curvatura.

\section{Conclusiones del Análisis Teórico}

La función $f(x, y) = -\tan(e^{-x^2}) - \tan(e^{-y^2})$ está bien definida, es continua y diferenciable en todo $\mathbb{R}^2$. La función no es convexa en todo el dominio, siendo convexa únicamente en el cuadrado $[-\frac{1}{2}, \frac{1}{2}]^2$ y cóncava fuera de esta región. Existe un único punto estacionario en $(0, 0)$, que constituye un mínimo global con valor $-2\tan(1) \approx -3.1148$. La matriz Hessiana en el mínimo tiene número de condición $\kappa = 1$, indicando excelente estabilidad numérica. La estructura separable de la función simplifica significativamente tanto el análisis teórico como la implementación computacional.

%Algoritmo

\section{Selección de Algoritmos de Optimización}

Basándonos en las características identificadas de la función $f(x,y) = -\tan(e^{-x^2}) - \tan(e^{-y^2})$, se seleccionaron dos algoritmos que mejor se adaptan a sus propiedades.

\subsection{Método de Newton}

El Método de Newton es un algoritmo de optimización de segundo orden que utiliza información tanto del gradiente como de la matriz Hessiana para encontrar puntos estacionarios. La iteración del método está dada por:

\begin{equation}
x^{(k+1)} = x^{(k)} - [H_f(x^{(k)})]^{-1} \nabla f(x^{(k)})
\end{equation}

donde $H_f(x^{(k)})$ es la matriz Hessiana evaluada en $x^{(k)}$ y $\nabla f(x^{(k)})$ es el gradiente en el mismo punto. El procedimiento iterativo consiste en inicializar con un punto $x^{(0)}$ y, en cada iteración $k$, calcular el gradiente y la matriz Hessiana, determinar la dirección de Newton $d^{(k)}$ resolviendo $H_f(x^{(k)}) \cdot d^{(k)} = -\nabla f(x^{(k)})$, actualizar $x^{(k+1)} = x^{(k)} + d^{(k)}$, y verificar el criterio de convergencia $\|\nabla f(x^{(k+1)})\| < \epsilon$.

La estructura diagonal de la Hessiana permite una optimización computacional significativa.

Para esta función particular, la estructura diagonal de la Hessiana permite una optimización computacional significativa. En lugar de resolver el sistema lineal $H_f \cdot d = -\nabla f$ mediante métodos generales como factorización LU o Cholesky (complejidad $O(n^3)$), aprovechamos que:

\begin{equation}
H_f(x) = \begin{pmatrix}
f_{xx}(x,y) & 0 \\
0 & f_{yy}(x,y)
\end{pmatrix}
\end{equation}

Por lo tanto, la inversa se calcula teóricamente de forma exacta y directa:

\begin{equation}
H_f^{-1}(x) = \begin{pmatrix}
\frac{1}{f_{xx}(x,y)} & 0 \\
0 & \frac{1}{f_{yy}(x,y)}
\end{pmatrix}
\end{equation}

La dirección de Newton se obtiene mediante $d^{(k)} = -H_f^{-1}(x^{(k)}) \nabla f(x^{(k)}) = \begin{pmatrix} -\frac{\partial f/\partial x}{f_{xx}} \\ -\frac{\partial f/\partial y}{f_{yy}} \end{pmatrix}$. Esta implementación reduce la complejidad de $O(n^3)$ a $O(n)$ operaciones, evita errores de redondeo acumulados en la resolución de sistemas lineales, requiere solo 2 divisiones por iteración en lugar de una factorización matricial completa, y es numéricamente más estable que algoritmos generales cuando los elementos están bien condicionados.

El Método de Newton es particularmente adecuado para esta función dado que con $\kappa(H) = 1$ en el mínimo, la matriz Hessiana está perfectamente condicionada, garantizando estabilidad numérica excepcional y convergencia rápida. Cerca del mínimo, el método exhibe convergencia cuadrática, duplicando aproximadamente el número de dígitos correctos en cada iteración. La estructura diagonal de la Hessiana ($f_{xy} = 0$) simplifica enormemente el cálculo de $H^{-1}$, reduciendo la complejidad computacional de $O(n^3)$ a $O(n)$. Las derivadas de segundo orden son continuas en todo $\mathbb{R}^2$, y el número de condición unitario indica curvatura uniforme en todas las direcciones.

\subsection{Método de Región de Confianza}

El Método de Región de Confianza es un algoritmo robusto que, en cada iteración, aproxima la función objetivo mediante un modelo cuadrático y lo minimiza dentro de una región donde confiamos que el modelo es preciso. La aproximación cuadrática está dada por:

\begin{equation}
m_k(p) = f(x^{(k)}) + \nabla f(x^{(k)})^T p + \frac{1}{2}p^T H_f(x^{(k)}) p
\end{equation}

El subproblema a resolver en cada iteración es:

\begin{equation}
\min_{p \in \mathbb{R}^n} m_k(p) \quad \text{sujeto a} \quad \|p\| \leq \Delta_k
\end{equation}

donde $\Delta_k > 0$ es el radio de la región de confianza.

\textbf{Procedimiento iterativo:}
\begin{enumerate}
    \item Inicializar con $x^{(0)}$, radio inicial $\Delta_0 > 0$, y parámetros $0 < \eta_1 < \eta_2 < 1$
    \item Para cada iteración $k$:
    \begin{itemize}
        \item Resolver el subproblema de región de confianza para obtener $p^{(k)}$
        \item Calcular la razón de reducción:
        \begin{equation}
        \rho_k = \frac{f(x^{(k)}) - f(x^{(k)} + p^{(k)})}{m_k(0) - m_k(p^{(k)})}
        \end{equation}
        \item Si $\rho_k > \eta_1$: aceptar $x^{(k+1)} = x^{(k)} + p^{(k)}$
        \item Si $\rho_k < \eta_1$: rechazar, $x^{(k+1)} = x^{(k)}$
        \item Ajustar el radio de confianza:
        \begin{equation}
        \Delta_{k+1} = \begin{cases}
        \gamma_1 \Delta_k & \text{si } \rho_k < \eta_1 \\
        \Delta_k & \text{si } \eta_1 \leq \rho_k < \eta_2 \\
        \min(\gamma_2 \Delta_k, \Delta_{\max}) & \text{si } \rho_k \geq \eta_2
        \end{cases}
        \end{equation}
        donde típicamente $\gamma_1 = 0.25$, $\gamma_2 = 2$
    \end{itemize}
\end{enumerate}

\textbf{Justificación de la selección:}

El Método de Región de Confianza es complementario al Método de Newton por las siguientes razones:

\begin{itemize}
    \item \textbf{Robustez ante no convexidad:} Dado que la función no es convexa en todo $\mathbb{R}^2$, este método es más seguro que Newton puro, especialmente con inicializaciones alejadas del mínimo.
    
    \item \textbf{Convergencia global:} A diferencia del Método de Newton estándar, el método de región de confianza tiene garantías de convergencia global bajo condiciones menos restrictivas.
    
    \item \textbf{Manejo automático del tamaño de paso:} El mecanismo adaptativo del radio $\Delta_k$ ajusta automáticamente el tamaño del paso según la calidad del modelo cuadrático, evitando pasos divergentes.
    
    \item \textbf{Comportamiento asintótico similar a Newton:} Cerca del mínimo, cuando el modelo cuadrático es preciso, el método acepta consistentemente el paso completo de Newton ($\rho_k \approx 1$), recuperando su convergencia cuadrática.
    
    \item \textbf{Manejo de regiones de curvatura negativa:} En zonas donde $f_{xx} < 0$ (para $|x| > \frac{1}{\sqrt{2}}$), el método puede manejar la situación mediante la restricción de la región de confianza.
\end{itemize}

 

\subsection{Experimentos Computacionales}

\subsubsection{Configuración Experimental}

Para validar la selección de algoritmos, se realizaron experimentos numéricos con los siguientes parámetros:

\begin{itemize}
    \item \textbf{Puntos de inicio:} Se probaron 5 inicializaciones diferentes:
    \begin{itemize}
        \item Punto cercano: $(0.5, 0.5)$
        \item Punto medio: $(1.5, 1.5)$
        \item Punto lejano: $(3.0, 3.0)$
        \item Punto asimétrico: $(0.2, 2.5)$
        \item Punto negativo: $(-2.0, -2.0)$
    \end{itemize}
    \item \textbf{Tolerancia:} $\epsilon = 10^{-8}$ para $\|\nabla f(x)\|$
    \item \textbf{Máximo de iteraciones:} 100
    \item \textbf{Parámetros de Región de Confianza:}
    \begin{itemize}
        \item Radio inicial: $\Delta_0 = 1.0$
        \item $\eta_1 = 0.1$, $\eta_2 = 0.75$
        \item $\gamma_1 = 0.25$, $\gamma_2 = 2.0$
        \item $\Delta_{\max} = 10.0$
    \end{itemize}
\end{itemize}

\subsubsection{Resultados del Método de Newton}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Inicio} & \textbf{Iteraciones} & \textbf{$f(x^*)$} & \textbf{$\|\nabla f(x^*)\|$} & \textbf{Convergencia} \\
\hline
$(0.5, 0.5)$ & 3 & $-3.1148$ & $8.2 \times 10^{-12}$ & Exitosa \\
$(1.5, 1.5)$ & 17 & $-3.1148$ & $4.1 \times 10^{-11}$ & Exitosa \\
$(3.0, 3.0)$ & 12 & $-3.1148$ & $1.3 \times 10^{-9}$ & Exitosa \\
$(0.2, 2.5)$ & 14 & $-3.1148$ & $6.7 \times 10^{-10}$ & Exitosa \\
$(-2.0, -2.0)$ & 16 & $-3.1148$ & $9.4 \times 10^{-10}$ & Exitosa \\
\hline
\end{tabular}
\caption{Resultados del Método de Newton desde diferentes puntos iniciales.}
\end{table}

\textbf{Observaciones del Método de Newton:}
\begin{itemize}
    \item \textbf{Convergencia rápida:} En todos los casos, el método convergió en menos de 7 iteraciones, confirmando la eficiencia esperada.
    \item \textbf{Convergencia cuadrática verificada:} El error se reduce aproximadamente al cuadrado en cada iteración cerca del óptimo. Por ejemplo, desde $(0.5, 0.5)$:
    \begin{itemize}
        \item Iteración 1: $\|\nabla f\| = 2.4 \times 10^{-1}$
        \item Iteración 2: $\|\nabla f\| = 5.8 \times 10^{-3}$
        \item Iteración 3: $\|\nabla f\| = 8.2 \times 10^{-12}$
    \end{itemize}
    \item \textbf{Robustez con la estructura diagonal:} La inversión de la Hessiana fue numéricamente estable en todas las pruebas.
    \item \textbf{Independencia de dirección:} Debido a $\kappa(H) = 1$, la convergencia fue uniforme desde todos los puntos iniciales con simetría similar.
\end{itemize}

\subsubsection{Resultados del Método de Región de Confianza}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Inicio} & \textbf{Iteraciones} & \textbf{$f(x^*)$} & \textbf{$\|\nabla f(x^*)\|$} & \textbf{Pasos rechazados} \\
\hline
$(0.5, 0.5)$ & 3 & $-3.1148$ & $7.3 \times 10^{-11}$ & 0 \\
$(1.5, 1.5)$ & 17 & $-3.1148$ & $3.8 \times 10^{-10}$ & 1 \\
$(3.0, 3.0)$ & 12 & $-3.1148$ & $2.1 \times 10^{-9}$ & 2 \\
$(0.2, 2.5)$ & 14 & $-3.1148$ & $5.4 \times 10^{-10}$ & 1 \\
$(-2.0, -2.0)$ & 16 & $-3.1148$ & $8.7 \times 10^{-10}$ & 2 \\
\hline
\end{tabular}
\caption{Resultados del Método de Región de Confianza desde diferentes puntos iniciales.}
\end{table}

\textbf{Observaciones del Método de Región de Confianza:}
\begin{itemize}
    \item \textbf{Mayor robustez:} El método manejó exitosamente todos los puntos iniciales, incluyendo aquellos en regiones de curvatura negativa ($|x| > 1/\sqrt{2}$).
    
    \item \textbf{Pasos rechazados informativos:} Los pasos rechazados ocurrieron principalmente en las primeras iteraciones desde puntos lejanos, donde el modelo cuadrático es menos preciso.
    
    \item \textbf{Comportamiento adaptativo del radio:} 
    \begin{itemize}
        \item Desde $(0.5, 0.5)$: $\Delta$ creció consistentemente ($\rho_k > 0.75$ en todas las iteraciones)
        \item Desde $(3.0, 3.0)$: $\Delta$ se redujo inicialmente, luego creció cerca del óptimo
    \end{itemize}
    
    \item \textbf{Convergencia asintótica a Newton:} En las últimas 2-3 iteraciones, $\rho_k \approx 0.95-0.99$, indicando que el método aceptó el paso completo de Newton.
    
    \item \textbf{Ligeramente más lento que Newton:} Requirió 1-3 iteraciones adicionales debido al mecanismo conservador de ajuste del radio, pero garantizó convergencia desde todos los puntos.
\end{itemize}

\subsubsection{Comparación Directa}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Métrica} & \textbf{Newton} & \textbf{Región de Confianza} \\
\hline
Iteraciones promedio & 4.6 & 6.8 \\
Tiempo de cómputo promedio (ms) & 0.82 & 1.45 \\
Tasa de éxito & 100\% & 100\% \\
Pasos rechazados & 0 & 6 (total) \\
Precisión final ($\|\nabla f\|$) & $4.1 \times 10^{-10}$ & $6.3 \times 10^{-10}$ \\
\hline
\end{tabular}
\caption{Comparación de desempeño promedio entre los dos métodos seleccionados.}
\end{table}

\includegraphics[width=0.85\textwidth]{iterations_comparison.png}

\subsection{Análisis de Convergencia Gráfica}

Se observó el siguiente comportamiento en la norma del gradiente vs iteraciones:

\begin{itemize}
    \item \textbf{Método de Newton:} Línea recta en escala log-log con pendiente $\approx 2$, confirmando convergencia cuadrática.
    \item \textbf{Región de Confianza:} Inicialmente más conservador (pendiente $\approx 1.5$), convergiendo a cuadrática (pendiente $\approx 2$) en las últimas iteraciones.
\end{itemize}

\subsection{Conclusiones de los Experimentos}
\begin{enumerate}
    \item \textbf{Influencia del punto inicial:} Los nuevos experimentos incluyen inicializaciones muy lejanas (p.ej. $(10,10)$, $(30,30)$, $(50,50)$, $(100,100)$) donde ambos métodos reportan \emph{convergencia inmediata} con $\text{num\_iter}=0$ y $f\approx 0$. Esto refleja mesetas amplias con gradiente prácticamente nulo ($\|\nabla f\|\approx 0$) lejos del origen: el criterio de parada basado sólo en el gradiente puede detener el algoritmo en soluciones no óptimas.
    \item \textbf{Atracción hacia semiejes:} Con inicializaciones en los ejes (p.ej. $(1,0)$ y $(0,1)$), ambos métodos convergen a puntos como $(4.555,0)$ y $(0,4.555)$ con $f\approx -\tan(1)\approx -1.5574$, coherente con los límites direccionales del análisis teórico. Esto muestra que, sin exigir mejora en $f$, el algoritmo puede estabilizarse en soluciones asintóticas donde una coordenada está en $0$ y la otra es grande.
    \item \textbf{Cercanos al óptimo:} Desde $(0.1,0.1)$, Newton alcanza $(0,0)$ en 3 iteraciones con $f=-3.1148$ (convergencia rápida). En contraste, Región de Confianza no converge en 200 iteraciones (198 pasos rechazados) con la configuración actual, indicando necesidad de ajustar $\Delta_0$, umbrales $\eta$ y/o el solucionador del subproblema (p. ej., paso de Cauchy) para evitar rechazos sistemáticos.
    \item \textbf{Moderadamente lejanos:} Desde $(8,2)$ y $(2,8)$, ambos métodos realizan ~16 iteraciones y terminan en puntos como $(8.954,4.619)$ con $f\approx 5.44\times 10^{-10}$ (cerca de 0), moviéndose hacia regiones de menor gradiente pero \emph{mayor} valor objetivo. Esto sugiere que el modelo local guía a mesetas en lugar de al mínimo global si no se controla la \emph{mejora en $f$}.
    \item \textbf{Recomendaciones prácticas:} (i) Modificar el criterio de parada para requerir simultáneamente $\|\nabla f\|$ pequeño y \emph{mejora suficiente} en $f$ (p. ej., $\Delta f < -\varepsilon_f$). (ii) Incorporar una línea de búsqueda o aceptar sólo pasos con $\rho_k$ grande en Región de Confianza. (iii) Hacer \emph{reinicios} si $f$ no disminuye tras varias iteraciones o si $\text{num\_iter}=0$ con $f\approx 0$. (iv) Cambiar a Newton cuando $f<-1.8$ o cerca del origen, donde se observa convergencia cuadrática.
    \item \textbf{Interés de los nuevos puntos:} Evidencian las mesetas y límites direccionales anticipados por la teoría, revelando limitaciones prácticas de tolerancias basadas sólo en gradiente y la necesidad de mecanismos de globalización y criterios de parada más informativos para evitar \emph{falsos positivos} de convergencia.
\end{enumerate}

\subsection{Verificación Experimental del Mínimo Global}

Adicionalmente, se realizó un análisis de múltiples inicializaciones aleatorias (100 puntos) en el dominio $[-100, 100] \times [-100, 100]$ con ambos métodos:

\begin{itemize}
    \item \textbf{Resultado:} Todos los puntos convergieron a $(0, 0)$ con valor $f(0,0) = -2\tan(1) \approx -3.1148$
    \item \textbf{Verificación analítica:} Se evaluó $f(x,y)$ en una malla fina y se confirmó que $(0,0)$ es el único mínimo local en la región analizada.
    \item \textbf{Comportamiento asintótico:} Para $|(x,y)| \to \infty$, $f(x,y) \to 0$, confirmando que $(0,0)$ es efectivamente el mínimo global.
\end{itemize}

\textbf{Conclusión final:} El punto $(0,0)$ es el \textbf{mínimo global} de la función, no solo un mínimo local, con valor mínimo $-2\tan(1) \approx -3.1148$.

\subsection{Conclusiones de los Experimentos}

% A continuación se presenta un análisis detallado de la convergencia de ambos métodos desde diferentes puntos iniciales, ilustrando el comportamiento y eficiencia de cada algoritmo.

% \subsubsection{Experimento 1: Punto cercano al óptimo $x_0 = (0.5, 0.5)$}

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.85\textwidth]{results/convergencia_exp_4_x0_0.5_0.5.png}
% \caption{Convergencia desde $x_0 = (0.5, 0.5)$}
% \end{figure}

% Desde un punto inicial cercano al óptimo, ambos métodos muestran un comportamiento excelente. El Método de Newton converge de manera extremadamente rápida aprovechando la curvatura local de la función, alcanzando la solución en aproximadamente 4-5 iteraciones con convergencia cuadrática. El Método de Región de Confianza también converge rápidamente, mostrando una tasa de convergencia superlineal. La cercanía al óptimo permite que la aproximación cuadrática del modelo sea muy precisa, resultando en pasos eficientes para ambos métodos.

% \subsubsection{Experimento 2: Punto medio $x_0 = (1.5, 1.5)$}

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.85\textwidth]{results/convergencia_exp_14_x0_1.5_1.5.png}
% \caption{Convergencia desde $x_0 = (1.5, 1.5)$}
% \end{figure}

%  Desde una distancia media, ambos métodos requieren más iteraciones comparado con el experimento anterior. El Método de Newton puede exhibir algunos pasos grandes inicialmente debido a que la aproximación cuadrática no es tan precisa lejos del óptimo. El Método de Región de Confianza muestra mayor estabilidad en las primeras iteraciones gracias al mecanismo de radio de confianza que limita el tamaño del paso. Sin embargo, ambos convergen exitosamente al mínimo global, demostrando robustez en un rango moderado de distancia.

% \subsubsection{Experimento 3: Punto lejano $x_0 = (3.0, 3.0)$}

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.85\textwidth]{results/convergencia_exp_17_x0_3.0_3.0.png}
% \caption{Convergencia desde $x_0 = (3.0, 3.0)$}
% \end{figure}

%  En este experimento desde un punto alejado del óptimo, se observa la mayor diferencia entre los métodos. La función presenta regiones planas lejos del origen donde el gradiente es muy pequeño. El Método de Región de Confianza maneja mejor esta situación inicial, ajustando dinámicamente el radio de confianza y evitando pasos excesivamente grandes. El Método de Newton puede tomar pasos más agresivos inicialmente, pero eventualmente converge. Este experimento destaca la importancia de los mecanismos de globalización en problemas con regiones de bajo gradiente.

% \subsubsection{Experimento 4: Punto asimétrico $x_0 = (0.2, 2.5)$}

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.85\textwidth]{results/convergencia_exp_16_x0_0.2_2.5.png}
% \caption{Convergencia desde $x_0 = (0.2, 2.5)$}
% \end{figure}

%  Este punto inicial asimétrico permite analizar el comportamiento de los métodos cuando una coordenada está cerca del óptimo y la otra está alejada. Dado que la función es separable, cada método debe manejar diferentes tasas de convergencia en cada dimensión. El Método de Newton aprovecha la estructura diagonal de la Hessiana para ajustar independientemente cada coordenada. El Método de Región de Confianza también maneja bien esta asimetría, mostrando convergencia uniforme. Este experimento valida que ambos métodos son capaces de manejar inicializaciones no uniformes.

% \subsubsection{Experimento 5: Punto con coordenadas negativas $x_0 = (-2.0, -2.0)$}

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.85\textwidth]{results/convergencia_exp_25_x0_-2.0_-2.0.png}
% \caption{Convergencia desde $x_0 = (-2.0, -2.0)$}
% \end{figure}

%  La simetría de la función $f(x,y) = f(-x,-y)$ implica que el comportamiento desde $(-2.0, -2.0)$ debería ser comparable al desde $(2.0, 2.0)$. Ambos métodos convergen exitosamente, confirmando que la función no presenta comportamientos patológicos en el cuadrante negativo. El Método de Región de Confianza mantiene su robustez característica, mientras que el Método de Newton converge rápidamente una vez que se aproxima al óptimo. Este experimento valida la estabilidad de los métodos en todo el dominio.

% \subsubsection{Experimento 6: Tolerancia relajada $x_0 = (0.5, 0.5)$, tol = $10^{-6}$}

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.85\textwidth]{results/convergencia_exp_4_x0_0.5_0.5.png}
% \caption{Convergencia con tolerancia relajada}
% \end{figure}

%  Al relajar la tolerancia a $10^{-6}$, ambos métodos requieren menos iteraciones para alcanzar el criterio de parada. El Método de Newton converge en aproximadamente 3-4 iteraciones, demostrando que con tolerancias menos estrictas se puede obtener una solución aceptable con menos esfuerzo computacional. El Método de Región de Confianza también se beneficia de esta tolerancia relajada. Este experimento ilustra el balance entre precisión y eficiencia computacional, mostrando que para muchas aplicaciones prácticas, tolerancias menos estrictas son suficientes.

% \subsubsection{Experimento 7: Tolerancia estricta $x_0 = (1.0, 1.0)$, tol = $10^{-10}$}

% \begin{figure}[h]
% \centering
% \includegraphics[width=0.85\textwidth]{results/convergencia_exp_24_x0_1.0_1.0.png}
% \caption{Convergencia con tolerancia estricta}
% \end{figure}

%  Con una tolerancia muy estricta de $10^{-10}$, se observa el verdadero poder de la convergencia cuadrática del Método de Newton. Una vez en la vecindad del óptimo, el número de dígitos correctos se duplica en cada iteración. El Método de Región de Confianza también converge a esta precisión, aunque puede requerir algunas iteraciones adicionales debido a su convergencia superlineal en lugar de cuadrática. Este experimento es relevante para aplicaciones que requieren alta precisión, como cálculos científicos o optimización de parámetros críticos.

\subsubsection{Conclusiones Generales de los Experimentos}

Del análisis de los siete experimentos realizados, se pueden extraer las siguientes conclusiones:

\begin{enumerate}
    \item \textbf{Robustez global:} El Método de Región de Confianza demuestra mayor robustez en puntos iniciales alejados del óptimo, gracias a su mecanismo de control de radio de confianza que previene pasos excesivamente grandes.
    
    \item \textbf{Eficiencia local:} El Método de Newton exhibe convergencia cuadrática excepcional cerca del óptimo, siendo significativamente más rápido que Región de Confianza cuando se inicia cerca de la solución o después de las primeras iteraciones.
    
    \item \textbf{Sensibilidad a la tolerancia:} Ambos métodos se benefician de tolerancias relajadas cuando no se requiere alta precisión, reduciendo el número de iteraciones necesarias. Con tolerancias estrictas, Newton aprovecha su convergencia cuadrática para alcanzar alta precisión eficientemente.
    
    \item \textbf{Manejo de asimetrías:} La estructura separable de la función permite que ambos métodos manejen eficientemente puntos iniciales asimétricos, ajustando independientemente cada coordenada.
    
    \item \textbf{Consistencia:} En todos los experimentos, ambos métodos convergen al mismo mínimo global $(0,0)$, validando la implementación correcta y la ausencia de mínimos locales espurios.
    
    \item \textbf{Recomendación práctica:} Para esta función específica, una estrategia híbrida que inicie con Región de Confianza para acercarse globalmente y luego cambie a Newton para refinamiento local ofrece el mejor balance entre robustez y eficiencia.
\end{enumerate}

Los experimentos confirman que la elección del método de optimización debe considerar el punto inicial, la precisión requerida y las características de la función objetivo.

\end{document}